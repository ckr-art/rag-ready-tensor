{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84be7331-c6c4-488f-b8c1-29023b483131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing files in rag_proj\\data\n",
      "Prepared 1 chunk(s) from files in rag_proj\\data\n",
      "Created Chroma collection 'rag_demo_collection' with 1 items (in-memory)\n",
      "Loading generator model 'google/flan-t5-small' on cpu (may download once)...\n",
      "\n",
      "=== Demo Q&A (answers + sources) ===\n",
      "\n",
      "Q: What is this publication about?\n",
      "A: Science/Tech\n",
      "Sources: lesson1.txt\n",
      "\n",
      "Q: Which tools are recommended in the documents?\n",
      "A: AAIDC RAG Assistant Project\n",
      "Sources: lesson1.txt\n",
      "\n",
      "Q: What limitation is mentioned?\n",
      "A: filename1>\n",
      "Sources: lesson1.txt\n"
     ]
    }
   ],
   "source": [
    "# ONE-CELL: Local RAG demo using chromadb + sentence-transformers + transformers (no langchain)\n",
    "# Paste & run in Jupyter. Uses packages you already have: chromadb, sentence-transformers, transformers, torch, nltk.\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Ensure NLTK punkt is available (fixes your earlier LookupError)\n",
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# --- Chroma + embedding function (uses sentence-transformers under the hood)\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# --- Transformers for local generation\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# --- Paths and basic setup\n",
    "ROOT = \"rag_proj\"\n",
    "DATA_DIR = Path(ROOT) / \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --- Create sample documents if none exist\n",
    "if not any(DATA_DIR.iterdir()):\n",
    "    sample_texts = [\n",
    "        (\"publication1.txt\", \n",
    "         \"Title: AAIDC RAG Assistant Project\\n\\n\"\n",
    "         \"This publication explains how to build a Retrieval-Augmented Generation (RAG) assistant using vector databases like Chroma. \"\n",
    "         \"It describes chunking documents, creating embeddings, and answering questions using retrieved context. \"\n",
    "         \"Limitations mention that small datasets are used for demo and retrieval accuracy depends on chunking and embedding quality.\"),\n",
    "        (\"publication2.txt\",\n",
    "         \"Title: Example Models and Tools\\n\\n\"\n",
    "         \"This short doc lists tools: Chroma (vector DB), sentence-transformers (embeddings), and Flan-T5 (local generation). \"\n",
    "         \"It also notes to cite sources when answering.\")\n",
    "    ]\n",
    "    for fname, txt in sample_texts:\n",
    "        (DATA_DIR / fname).write_text(txt, encoding=\"utf-8\")\n",
    "    print(\"Created sample documents in\", DATA_DIR)\n",
    "else:\n",
    "    print(\"Using existing files in\", DATA_DIR)\n",
    "\n",
    "# --- Simple safe sentence-based chunker\n",
    "def chunk_text(text, max_words=300, overlap_words=50):\n",
    "    sents = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    cur = []\n",
    "    cur_count = 0\n",
    "    for sent in sents:\n",
    "        w = len(sent.split())\n",
    "        if cur_count + w <= max_words or not cur:\n",
    "            cur.append(sent)\n",
    "            cur_count += w\n",
    "        else:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            # create overlap\n",
    "            overlap = \" \".join(\" \".join(cur).split()[-overlap_words:]) if overlap_words>0 else \"\"\n",
    "            cur = [overlap] if overlap else []\n",
    "            cur.append(sent)\n",
    "            cur_count = len(\" \".join(cur).split())\n",
    "    if cur:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    return chunks\n",
    "\n",
    "# --- Read files and prepare lists for Chroma\n",
    "docs_texts = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "for f in sorted(DATA_DIR.iterdir()):\n",
    "    if not f.is_file() or f.suffix.lower() not in [\".txt\", \".md\", \".json\"]:\n",
    "        continue\n",
    "    raw = f.read_text(encoding=\"utf-8\").strip()\n",
    "    if not raw:\n",
    "        continue\n",
    "    if f.suffix.lower() == \".json\":\n",
    "        try:\n",
    "            jobj = json.loads(raw)\n",
    "            text = jobj.get(\"text\") or jobj.get(\"body\") or raw\n",
    "        except Exception:\n",
    "            text = raw\n",
    "    else:\n",
    "        text = raw\n",
    "    chunks = chunk_text(text, max_words=300, overlap_words=50)\n",
    "    for i, c in enumerate(chunks):\n",
    "        _id = f\"{f.stem}_chunk_{i}\"\n",
    "        docs_texts.append(c)\n",
    "        metadatas.append({\"source_file\": f.name, \"chunk_id\": _id})\n",
    "        ids.append(_id)\n",
    "\n",
    "print(f\"Prepared {len(docs_texts)} chunk(s) from files in {DATA_DIR}\")\n",
    "\n",
    "if not docs_texts:\n",
    "    raise RuntimeError(\"No documents or chunks found. Add .txt files to rag_proj/data and re-run this cell.\")\n",
    "\n",
    "# --- Create Chroma client and collection using SentenceTransformer embeddings\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBED_MODEL)\n",
    "\n",
    "client = chromadb.Client()  # in-memory demo client (no persistent dir)\n",
    "COLL_NAME = \"rag_demo_collection\"\n",
    "# Remove existing collection with same name to avoid duplicates (safe for demo)\n",
    "try:\n",
    "    client.delete_collection(name=COLL_NAME)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "collection = client.create_collection(name=COLL_NAME, embedding_function=ef)\n",
    "\n",
    "collection.add(documents=docs_texts, metadatas=metadatas, ids=ids)\n",
    "print(f\"Created Chroma collection '{COLL_NAME}' with {collection.count()} items (in-memory)\")\n",
    "\n",
    "# --- Retrieval helper\n",
    "def retrieve_top_k(query, k=3):\n",
    "    results = collection.query(query_texts=[query], n_results=k, include=[\"documents\",\"metadatas\",\"distances\"])\n",
    "    docs = results.get(\"documents\", [[]])[0]\n",
    "    metas = results.get(\"metadatas\", [[]])[0]\n",
    "    dists = results.get(\"distances\", [[]])[0]\n",
    "    out = []\n",
    "    for doc, meta, dist in zip(docs, metas, dists):\n",
    "        out.append({\"text\": doc, \"metadata\": meta, \"distance\": dist})\n",
    "    return out\n",
    "\n",
    "# --- Load generator model (Flan-T5 small)\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Loading generator model '{MODEL_NAME}' on {device} (may download once)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# --- RAG QA function: retrieve -> build limited context -> generate\n",
    "def ask_question(question, k=3, max_context_chunks=3, max_new_tokens=128):\n",
    "    retrieved = retrieve_top_k(question, k=k)\n",
    "    if not retrieved:\n",
    "        return \"I don't know â€” no relevant documents found.\", []\n",
    "    # build context from top-n retrieved chunks (limit size to avoid tokenizer overflow)\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "    for r in retrieved[:max_context_chunks]:\n",
    "        txt = r[\"text\"].strip()\n",
    "        # limit chunk text length (words) to keep prompt small\n",
    "        words = txt.split()\n",
    "        if len(words) > 250:\n",
    "            txt = \" \".join(words[:250]) + \" ...\"\n",
    "        src = r[\"metadata\"].get(\"source_file\", \"unknown\")\n",
    "        cid = r[\"metadata\"].get(\"chunk_id\", \"unknown\")\n",
    "        context_parts.append(f\"Source: {src} | Chunk: {cid}\\n{txt}\")\n",
    "        if src not in sources:\n",
    "            sources.append(src)\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    prompt = (\n",
    "        \"You are an assistant that must answer the question using ONLY the context below. \"\n",
    "        \"If the answer is not in the context, reply exactly: \\\"I don't know â€” the information is not in the documents.\\\"\\n\\n\"\n",
    "        f\"CONTEXT:\\n{context}\\n\\nQUESTION: {question}\\n\\nAnswer concisely (1-3 sentences). Then on a new line list the sources used as: Sources: <filename1>, <filename2>.\"\n",
    "    )\n",
    "    # Tokenize + generate (truncate if prompt too long)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    gen = gen_model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=4, early_stopping=True)\n",
    "    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "    return out.strip(), sources\n",
    "\n",
    "# --- Quick demo queries\n",
    "demo_qs = [\n",
    "    \"What is this publication about?\",\n",
    "    \"Which tools are recommended in the documents?\",\n",
    "    \"What limitation is mentioned?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Demo Q&A (answers + sources) ===\")\n",
    "for q in demo_qs:\n",
    "    ans, srcs = ask_question(q)\n",
    "    print(\"\\nQ:\", q)\n",
    "    print(\"A:\", ans)\n",
    "    print(\"Sources:\", \", \".join(srcs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee095901-f0ea-4715-adeb-2c0b4e34cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Running RAG Demo & Saving Log...\n",
      "\n",
      "Q: What is this publication about?\n",
      "A: Science/Tech\n",
      "Sources: lesson1.txt \n",
      "\n",
      "Q: Which tools are recommended in the documents?\n",
      "A: LangChain and vector databases\n",
      "Sources: lesson1.txt \n",
      "\n",
      "Q: What limitation is mentioned?\n",
      "A: small dataset demo and offline vector DB precision challenges\n",
      "Sources: lesson1.txt \n",
      "\n",
      "ðŸ“‚ Demo log saved at: rag_proj\\output_demo.txt\n"
     ]
    }
   ],
   "source": [
    "# ---------------- SAVE RAG DEMO OUTPUT + LOGGING ----------------\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "import nltk\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Setup directories\n",
    "ROOT = \"rag_proj\"\n",
    "DATA_DIR = Path(ROOT) / \"data\"\n",
    "SAVE_LOG = Path(ROOT) / \"output_demo.txt\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Load existing Chroma collection\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "embedding_fn = SentenceTransformerEmbeddingFunction(model_name=EMBED_MODEL)\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(name=\"rag_demo_collection\", embedding_function=embedding_fn)\n",
    "\n",
    "# Retrieval helper\n",
    "def retrieve_top_k(query, k=3):\n",
    "    r = collection.query(query_texts=[query], n_results=k, include=[\"documents\",\"metadatas\",\"distances\"])\n",
    "    docs = r.get(\"documents\", [[]])[0]\n",
    "    metas = r.get(\"metadatas\", [[]])[0]\n",
    "    dists = r.get(\"distances\", [[]])[0]\n",
    "\n",
    "    # âœ… Corrected: use separate variables doc_text and dist_val\n",
    "    results = []\n",
    "    for doc_text, meta_val, dist_val in zip(docs, metas, dists):\n",
    "        results.append({\n",
    "            \"text\": doc_text.strip(),  # now it's always text âœ…\n",
    "            \"metadata\": meta_val,\n",
    "            \"distance\": dist_val       # always float âœ…\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Load generator model\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Chunker\n",
    "def chunk_text(text, max_words=300, overlap=50):\n",
    "    sents = sent_tokenize(text)\n",
    "    chunks, buf, count = [], [], 0\n",
    "    for s in sents:\n",
    "        w = len(s.split())\n",
    "        if count + w <= max_words or not buf:\n",
    "            buf.append(s); count += w\n",
    "        else:\n",
    "            chunks.append(\" \".join(buf))\n",
    "            prev = \" \".join(\" \".join(buf).split()[-overlap:]) if overlap>0 else \"\"\n",
    "            buf = [prev] if prev else []\n",
    "            buf.append(s); count = len(\" \".join(buf).split())\n",
    "    if buf: chunks.append(\" \".join(buf))\n",
    "    return chunks\n",
    "\n",
    "# QA function\n",
    "def ask(q, k=3, max_new=128):\n",
    "    top = retrieve_top_k(q, k=k)\n",
    "    ctx, srcs = [], []\n",
    "\n",
    "    for t in top[:3]:\n",
    "        txt = t[\"text\"]\n",
    "        if len(txt.split())>250:\n",
    "            txt = \" \".join(txt.split()[:250]) + \" ...\"\n",
    "        src = t[\"metadata\"].get(\"source_file\",\"unknown\")\n",
    "        ctx.append(f\"Source: {src}\\n{txt}\")\n",
    "        if src not in srcs:\n",
    "            srcs.append(src)\n",
    "\n",
    "    prompt = \"Answer using only context. If unknown say: I don't know.\\n\\nCONTEXT:\\n\" + \"\\n\\n---\\n\".join(ctx) + f\"\\n\\nQUESTION:\\n{q}\"\n",
    "    inp = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    gen = gen_model.generate(**inp, max_new_tokens=max_new)\n",
    "    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "    return out.strip(), srcs\n",
    "\n",
    "# Demo Queries\n",
    "demo_qs = [\n",
    "    \"What is this publication about?\",\n",
    "    \"Which tools are recommended in the documents?\",\n",
    "    \"What limitation is mentioned?\"\n",
    "]\n",
    "\n",
    "log_lines = [\"=== RAG Demo Output Log ===\\n\"]\n",
    "print(\"\\nâœ… Running RAG Demo & Saving Log...\\n\")\n",
    "\n",
    "for q in demo_qs:\n",
    "    ans, src = ask(q)\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", ans)\n",
    "    print(\"Sources:\", \", \".join(src), \"\\n\")\n",
    "    log_lines.append(f\"Q: {q}\\nA: {ans}\\nSources: {', '.join(src)}\\n\\n\")\n",
    "\n",
    "SAVE_LOG.write_text(\"\".join(log_lines))\n",
    "print(\"ðŸ“‚ Demo log saved at:\", SAVE_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05275b6c-3a06-4d92-a1ec-bb272559340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6187cca0-3149-4ee9-ae11-fd8be739fc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is this publication about?\n",
      "A: Science/Tech\n",
      "Sources: lesson1.txt \n",
      "\n",
      "Q: Which tools are recommended in the documents?\n",
      "A: LangChain and vector databases\n",
      "Sources: lesson1.txt \n",
      "\n",
      "Q: What limitation is mentioned?\n",
      "A: small dataset demo and offline vector DB precision challenges\n",
      "Sources: lesson1.txt \n",
      "\n",
      "ðŸ“‚ Demo log saved at: rag_proj\\output_demo.txt\n"
     ]
    }
   ],
   "source": [
    "log_lines = [\"=== RAG Demo Output Log ===\\n\"]\n",
    "\n",
    "for q in demo_qs:\n",
    "    ans, src = ask(q)\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", ans)\n",
    "    print(\"Sources:\", \", \".join(src), \"\\n\")\n",
    "    log_lines.append(f\"Q: {q}\\nA: {ans}\\nSources: {', '.join(src)}\\n\\n\")\n",
    "\n",
    "SAVE_LOG.write_text(\"\".join(log_lines))\n",
    "print(\"ðŸ“‚ Demo log saved at:\", SAVE_LOG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
